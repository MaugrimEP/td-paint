checkpoint_params:
  # TO RETRAIN FROM A .CKPT
  retrain_saved_path: /path/to/model.ckpt  # TODO
  retrain_retrain_from_checkpoint: load_weights

  dirpath: saved_models

args:  #guided_diffusion_params
  activation_checkpoint: false
  t_mode: map_vanilla
  t_strategy: condition
  input_mode: clean
  down_sample_strat: time_map
  generation_t_map: t_map_mask
  t_clean_value: 0
  patch_size_train: [1, 2, 4, 8, 16, 32, 64, 128, 256]
  patch_weight_train: [1, 1, 1, 1, 1, 1, 1, 1, 8]
  condition_proba: [0., 1.0]
  learn_the_condition: false

model_params:
  logging:
    # Logging of training steps
    log_steps:
      save_image_to_disk_stage: []
      stages: []

    # Perform unconditional generations logging
    log_generate_uncond:
      save_image_to_disk_stage: []
      stages: []

    # Perform conditional generations logging (inpainting)
    log_generate_cond:
      early_leave: false
      stages: [test]
      save_image_to_disk_stage: [test]

    # Perform conditional generations logging (inpainting) with diversity
    log_generate_diversity:
      early_leave: false
      save_image_to_disk_stage: [test]
      stages: [test]
      variation_quantity: 10  # number of different images to generate
      generate_all_in_batch: true

    # how many steps in the generative diffusion process to log, 1 = log only the last image
    time_step_in_process: 1

  optimizer:
    ema:
      perform_double_validation: false  # if true, then compute on the Normal model and on the EMA model
      validate_original_weights: false  # if perform_double_validation false, then validate the EMA model if True, else on the Normal model
  metrics:
    no_metrics: false  # activate the metrics

  torch_params:
    compile: false
    torch_float32_matmul_precision: medium  # [ medium | high | highest ]

trainer_params:
  devices: 1
  strategy: auto
  # auto to train on 1 GPU
  # ddp for multi gpu
  # ddp_find_unused_parameters_true to train with metrics available
  precision: bf16-mixed
  limit_train_batches: 0
  limit_val_batches: 0

  skip_training: true

wandb_params:
  project: ProjectName
  mode: offline
