---
checkpoint_params:
  # TO RETRAIN FROM A .CKPT
  # retrain_saved_path: /path/to/model.ckpt
  # retrain_retrain_from_checkpoint: load_weights  # just load the weights and reset PL training
  # retrain_retrain_from_checkpoint: load_train  # load a PL ckpt (also resume epochs, optimizer etc, to use to train during multiple runs)

  # how often to save and how many version to save
  on_epochs__save_top_k: 1_000
  on_epochs__every_n_epochs: 1
  on_steps__save_top_k: 1_000
  on_steps__every_n_steps: 30_000

args:  # guided_diffusion_params
  activation_checkpoint: true
  t_mode: map_vanilla
  t_strategy: condition
  input_mode: clean
  down_sample_strat: time_map
  generation_t_map: t_map_mask
  t_clean_value: 0
  patch_size_train: [1, 2, 4, 8, 16, 32, 64, 128, 256]
  patch_weight_train: [1, 1, 1, 1, 1, 1, 1, 1, 8]
  condition_proba: [0., 1.0]
  learn_the_condition: false

dataset_params:
  mask_params:
    lama_mask_proba: 0.6

model_params:
  logging:
    # Logging of training steps
    log_steps:
      early_leave: true
      save_image_to_disk_stage: [valid]
      stages: [valid]
      frequencies: [1, 1, 1]
      max_quantity: 20

    # Perform unconditional generations logging
    log_generate_uncond:
      early_leave: true
      save_image_to_disk_stage: [valid]
      stages: [valid]
      frequencies: [1, 1, 1]
      max_quantity: 20

    # Perform conditional generations logging (inpainting)
    log_generate_cond:
      early_leave: true
      stages: [valid, test]
      save_image_to_disk_stage: [valid, test]
      frequencies: [1, 1, 1]
      max_quantity: 20

    # Perform conditional generations logging (inpainting) with diversity
    log_generate_diversity:
      early_leave: true
      save_image_to_disk_stage: [test]
      stages: [test]
      frequencies: [1, 1, 1]
      max_quantity: 20
      variation_quantity: 5
      generate_all_in_batch: true

    # how many steps in the generative diffusion process to log, 1 = log only the last image
    time_step_in_process: 1

  metrics:
    no_metrics: false
    # activate the metrics, if True then trainer_params.strategy should be set to ddp_find_unused_parameters_true
    # when using multiples devices

trainer_params:
  devices: 1
  strategy: auto
  # auto to train on 1 GPU
  # ddp for multi gpu
  # ddp_find_unused_parameters_true to train with metrics available
  limit_val_batches: 4
  val_check_interval: null
  check_val_every_n_epoch: 1
  max_epochs: 1_000
  max_steps: -1
  num_sanity_val_steps: 0

wandb_params:
  mode: offline
